{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from utils import *\n",
    "from sbm_class import *\n",
    "from neal_batched import *\n",
    "from neal_sequential import *\n",
    "from batched import *\n",
    "from sequential import *\n",
    "from metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Approach:**\n",
    "\n",
    "We implement a convergence criterion based on the one proposed in the book \"Bayesian Data Analysis\" by Gelman and Carlin, the $\\hat{R}$ of page 285 (equation 11.4).\n",
    "\n",
    "Let us take situations with a strong recovery, and we measure for different $\\gamma$'s the convergence speed. We repeat this for networks increasing in size (notice that, by our proof, we are concerned with $\\frac{n}{k}$, not only $n$, so we may want to test if increasing the community size is relevant or not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_chains(chains, splits = 2):\n",
    "    # chains is a list of arrays\n",
    "    n_tot = len(chains[0])\n",
    "    if n_tot % splits != 0:\n",
    "        raise ValueError(\"The number of samples is not divisible by the number of splits\")\n",
    "    # split each chain into splits parts\n",
    "    split_chains = []\n",
    "    for chain in chains:\n",
    "        split_chain = np.array_split(chain, splits)\n",
    "        split_chains.append(split_chain)\n",
    "    return np.vstack(split_chains).T # row index: iteration, col_index: chain\n",
    "\n",
    "def R_hat(split_chains):\n",
    "    n = split_chains.shape[0]\n",
    "    m = split_chains.shape[1]\n",
    "    # within chains variance\n",
    "    W = np.mean(np.sum((split_chains - np.mean(split_chains, axis=0))**2, axis=0)/(n-1))\n",
    "    # between chains variance\n",
    "    B = n*np.sum((np.mean(split_chains, axis=0) - np.mean(split_chains))**2)/(m-1)\n",
    "    # estimated variance\n",
    "    var_plus = (n-1)/n*W + 1/n*B\n",
    "    return np.sqrt(var_plus/W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization parameter\n",
    "alpha = 0.2\n",
    "# set parameters for the network\n",
    "n = 400\n",
    "k = 4\n",
    "p = 0.17\n",
    "q = 0.08\n",
    "# set parameters for the samplers\n",
    "n_iter = 250\n",
    "burn_in = 50\n",
    "\n",
    "alpha_p = 1\n",
    "beta_p = 1\n",
    "alpha_q = 1\n",
    "beta_q = 1\n",
    "\n",
    "# changing parameters\n",
    "pi = np.ones(k) # deterministic prior\n",
    "gamma_list = [0, 0.1, 1, 10, 10000, 1000000]\n",
    "\n",
    "# seed\n",
    "np.random.seed(0)\n",
    "\n",
    "splits = 2\n",
    "n_chains = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbm = Homogeneous_SBM(n, k, p, q, Neal=False)\n",
    "A = sbm.get_A()\n",
    "z = sbm.get_z()\n",
    "# warm initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progressive_R_hat(estimand_array, splits = 2, n_chains = 4, n_iter=250, burn_in=0, ignore=0):\n",
    "\n",
    "    # compute R_hat for subsequence (1,r) for all r\n",
    "    R_hat_list = np.zeros(n_iter-burn_in//splits)\n",
    "\n",
    "    if not isinstance(estimand_array, np.ndarray):\n",
    "        estimand_array = np.array(estimand_array).T\n",
    "\n",
    "    for r in range(1+splits, n_iter-burn_in, splits):\n",
    "        l = int(r*ignore)\n",
    "        while ((r+1-l) % splits) != 0:\n",
    "            l = l - 1\n",
    "        # take the first r rows, possibly ignoring a portion of the samples\n",
    "        list_split = estimand_array[l:r+1, :]\n",
    "        # convert list_split from array to list of arrays (n_chains arrays)\n",
    "        list_split = [list_split[:, i] for i in range(n_chains)]\n",
    "        list_split = split_chains(list_split, splits)\n",
    "        R_hat_list[r//splits] = R_hat(list_split)\n",
    "        return R_hat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " if extra:\n",
    "    # run the samplers (n_chains)\n",
    "    # batched\n",
    "    batched_p_list = np.zeros((n_iter, n_chains))\n",
    "    batched_q_list = np.zeros((n_iter, n_chains))\n",
    "    for a in range(n_chains):\n",
    "        z_init = warm_initializer(z, alpha, n, k)\n",
    "        sampler = batched_Gibbs_sampler(A, z_init, alpha_p, beta_p, alpha_q, beta_q, pi_pri = pi)\n",
    "        sampler.run(n_iter)\n",
    "        # store the p_lists\n",
    "        batched_p_list[:, a] = sampler.get_p_list()\n",
    "        batched_q_list[:, a] = sampler.get_q_list()\n",
    "\n",
    "    # compute R_hat for subsequence (1,r) for all r\n",
    "    batched_R_hat_p = np.zeros(n_iter//2)\n",
    "    batched_R_hat_q = np.zeros(n_iter//2)\n",
    "\n",
    "    for r in range(1+splits, n_iter, 2):\n",
    "        # take the first r rows of the p_list and q_list\n",
    "        p_list_split = batched_p_list[:r+1, :]\n",
    "        q_list_split = batched_q_list[:r+1, :]\n",
    "        # convert p_list_split from array to list of arrays (n_chains arrays)\n",
    "        p_list_split = [p_list_split[:, i] for i in range(n_chains)]\n",
    "        q_list_split = [q_list_split[:, i] for i in range(n_chains)]\n",
    "        p_list_split = split_chains(p_list_split, splits)\n",
    "        q_list_split = split_chains(q_list_split, splits)\n",
    "        batched_R_hat_p[r//2] = R_hat(p_list_split)\n",
    "        batched_R_hat_q[r//2] = R_hat(q_list_split)\n",
    "\n",
    "    # plot the R_hat for p and q in separate subplots\n",
    "    plt.figure(figsize = (15,15))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(batched_R_hat_p[3:], label='R_hat_p')\n",
    "    plt.title('R_hat for p')\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(batched_R_hat_q[3:], label='R_hat_q')\n",
    "    plt.title('R_hat for q')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define a convergence threshold, say $\\hat{R}=0.99$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if extra:\n",
    "    # batched sampler\n",
    "    # run the samplers (n_chains)\n",
    "    # batched\n",
    "    batched_p_list = np.zeros((n_iter, n_chains))\n",
    "    batched_q_list = np.zeros((n_iter, n_chains))\n",
    "    loss_list = np.zeros((n_iter, n_chains))\n",
    "    for a in range(n_chains):\n",
    "        z_init = warm_initializer(z, alpha, n, k)\n",
    "        sampler = batched_Gibbs_sampler(A, z_init, alpha_p, beta_p, alpha_q, beta_q, pi_pri = pi)\n",
    "        sampler.run(n_iter)\n",
    "        # store the p_lists\n",
    "        batched_p_list[:, a] = sampler.get_p_list()\n",
    "        batched_q_list[:, a] = sampler.get_q_list()\n",
    "        z_list = sampler.get_z_list()\n",
    "        loss_list[:, a] = [loss(z, z_list[i])[0] for i in range(n_iter)]\n",
    "\n",
    "    # compute R_hat for subsequence (1,r) for all r\n",
    "    batched_R_hat_p = np.zeros(n_iter//2)\n",
    "    batched_R_hat_q = np.zeros(n_iter//2)\n",
    "    idx = np.zeros(n_iter//2)\n",
    "\n",
    "    for r in range(1+splits, n_iter, 2):\n",
    "        # take the first r rows of the p_list and q_list\n",
    "        p_list_split = batched_p_list[:r+1, :]\n",
    "        q_list_split = batched_q_list[:r+1, :]\n",
    "        # convert p_list_split from array to list of arrays (n_chains arrays)\n",
    "        p_list_split = [p_list_split[:, i] for i in range(n_chains)]\n",
    "        q_list_split = [q_list_split[:, i] for i in range(n_chains)]\n",
    "        p_list_split = split_chains(p_list_split, splits)\n",
    "        q_list_split = split_chains(q_list_split, splits)\n",
    "        batched_R_hat_p[r//2] = R_hat(p_list_split)\n",
    "        batched_R_hat_q[r//2] = R_hat(q_list_split)\n",
    "        idx[r//2] = r\n",
    "\n",
    "    # average the loss over the chains\n",
    "    loss_avg = np.mean(loss_list, axis=1)\n",
    "\n",
    "    # plot the R_hat for p and q in the same subplot, and the loss in a separate subplot\n",
    "    plt.figure(figsize = (15,15))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(idx[3:], batched_R_hat_p[3:], label='R_hat_p')\n",
    "    plt.plot(idx[3:], batched_R_hat_q[3:], label='R_hat_q')\n",
    "    plt.title('R_hat for p and q')\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(loss_avg, label='loss')\n",
    "    plt.title('loss')\n",
    "    if pic:\n",
    "        plt.savefig('r_hat/batched_R_hat_loss.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # all gamma values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_convergence(list, lb, ub):\n",
    "    candidates = []\n",
    "    for i in range(len(list)):\n",
    "        if list[i] > lb and list[i] < ub:\n",
    "            candidates.append(i)\n",
    "    for i in candidates:\n",
    "        # check if all the following values are within the bounds\n",
    "        if all([list[j] > lb and list[j] < ub for j in range(i+1, len(list))]):\n",
    "            return i\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for the network\n",
    "n = 500\n",
    "k = 4\n",
    "# set parameters for the samplers\n",
    "n_iter = 50\n",
    "burn_in = 0\n",
    "\n",
    "alpha_p = 1\n",
    "beta_p = 1\n",
    "alpha_q = 1\n",
    "beta_q = 1\n",
    "\n",
    "seed = 1\n",
    "\n",
    "# changing parameters\n",
    "pi = np.ones((n, k)) # deterministic prior\n",
    "# row normalization of pi\n",
    "pi = pi/pi.sum(axis=1)[:, None]\n",
    "gamma_list = [0, 0.1, 1, 10, 10000]\n",
    "\n",
    "param_list = [(0.17, 0.08, 0.2), (0.4, 0.2, 0.4)] #(p,q,alpha)\n",
    "# for each value in the list, we should assess convergence using the r_hat values on the loss\n",
    "# we do it for the batched sampler, the sequential one, the neal sequential ones wiht different gamma values and the neal batched ones with \n",
    "# different gamma values\n",
    "# we will store the values of r_hat for the loss\n",
    "splits = 2\n",
    "n_chains = 4\n",
    "\n",
    "lb = 0.925\n",
    "ub = 1.075"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 84.33it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 88.21it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 89.21it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 93.84it/s]\n",
      "C:\\Users\\feder\\AppData\\Local\\Temp\\ipykernel_44752\\1691313796.py:73: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_batched = pd.concat([df_batched, tmp], axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence for p=0.17, q=0.08, alpha=0.2: loss=20, p=11, q=17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 81.57it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 86.45it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 79.43it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 91.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence for p=0.4, q=0.2, alpha=0.4: loss=4, p=5, q=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_batched = pd.DataFrame(columns=['p', 'q', 'alpha', 'convergence_loss', 'convergence_p', 'convergence_q'])\n",
    "\n",
    "np.random.seed(seed)\n",
    "pic = False\n",
    "\n",
    "# run the samplers (n_chains)\n",
    "# batched\n",
    "batched_p_list = np.zeros((len(param_list), n_iter, n_chains))\n",
    "batched_q_list = np.zeros((len(param_list), n_iter, n_chains))\n",
    "loss_list = np.zeros((len(param_list), n_iter, n_chains))\n",
    "\n",
    "for (p,q,alpha) in param_list:\n",
    "    sbm = Homogeneous_SBM(n, k, p, q, Neal=False)\n",
    "    A = sbm.get_A()\n",
    "    z = sbm.get_z()\n",
    "    for a in range(n_chains):\n",
    "        z_init = warm_initializer(z, alpha, n, k)\n",
    "        sampler = batched_Gibbs_sampler(A, z_init, alpha_p, beta_p, alpha_q, beta_q, pi_pri = pi)\n",
    "        sampler.run(n_iter)\n",
    "        # store the p_lists\n",
    "        batched_p_list[param_list.index((p,q,alpha)), :, a] = sampler.get_p_list()\n",
    "        batched_q_list[param_list.index((p,q,alpha)), :, a] = sampler.get_q_list()\n",
    "        z_list = sampler.get_z_list()\n",
    "        loss_list[param_list.index((p,q,alpha)), :, a] = [loss(z, z_list[i])[0] for i in range(n_iter)]\n",
    "\n",
    "    # compute R_hat for subsequence (1,r) for all r\n",
    "    batched_R_hat_p = np.zeros(n_iter//2)\n",
    "    batched_R_hat_q = np.zeros(n_iter//2)\n",
    "    batched_R_hat_loss = np.zeros(n_iter//2)\n",
    "    idx = np.zeros(n_iter//2)\n",
    "\n",
    "    for r in range(1+splits, n_iter, 2):\n",
    "        # take the first r rows of the p_list and q_list\n",
    "        p_list_split = batched_p_list[param_list.index((p,q,alpha)),:r+1, :]\n",
    "        q_list_split = batched_q_list[param_list.index((p,q,alpha)),:r+1, :]\n",
    "        loss_list_split = loss_list[param_list.index((p,q,alpha)),:r+1, :]\n",
    "        # convert p_list_split from array to list of arrays (n_chains arrays)\n",
    "        p_list_split = [p_list_split[:, i] for i in range(n_chains)]\n",
    "        q_list_split = [q_list_split[:, i] for i in range(n_chains)]\n",
    "        loss_list_split = [loss_list_split[:, i] for i in range(n_chains)]\n",
    "        p_list_split = split_chains(p_list_split, splits)\n",
    "        q_list_split = split_chains(q_list_split, splits)\n",
    "        loss_list_split = split_chains(loss_list_split, splits)\n",
    "        batched_R_hat_p[r//2] = R_hat(p_list_split)\n",
    "        batched_R_hat_q[r//2] = R_hat(q_list_split)\n",
    "        batched_R_hat_loss[r//2] = R_hat(loss_list_split)\n",
    "        idx[r//2] = r\n",
    "\n",
    "    # average the loss over the chains\n",
    "    # loss_avg = np.mean(loss_list[param_list.index((p,q,alpha)), :, :], axis=1)\n",
    "    # print(loss_avg)\n",
    "\n",
    "    # plot the R_hat for p and q in the same subplot, and the loss in a separate subplot\n",
    "    # plt.figure(figsize = (15,15))\n",
    "    # plt.subplot(2, 1, 1)\n",
    "    # plt.plot(idx[3:], batched_R_hat_p[3:], label='R_hat_p')\n",
    "    # plt.plot(idx[3:], batched_R_hat_q[3:], label='R_hat_q')\n",
    "    # plt.title('R_hat for p and q')\n",
    "    # plt.subplot(2, 1, 2)\n",
    "    # plt.plot(batched_R_hat_loss[3:], label='loss')\n",
    "    # plt.title('loss')\n",
    "    # plt.show()\n",
    "\n",
    "    # compute the convergence\n",
    "    conv_loss = find_convergence(batched_R_hat_loss, lb, ub)\n",
    "    conv_p = find_convergence(batched_R_hat_p, lb, ub)\n",
    "    conv_q = find_convergence(batched_R_hat_q, lb, ub)\n",
    "\n",
    "    print(f'Convergence for p={p}, q={q}, alpha={alpha}: loss={conv_loss}, p={conv_p}, q={conv_q}')\n",
    "\n",
    "    tmp = pd.DataFrame({'p': p, 'q': q, 'alpha': alpha, 'convergence_loss': conv_loss, 'convergence_p': conv_p, 'convergence_q': conv_q}, index = [0])\n",
    "\n",
    "    df_batched = pd.concat([df_batched, tmp], axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>q</th>\n",
       "      <th>alpha</th>\n",
       "      <th>convergence_loss</th>\n",
       "      <th>convergence_p</th>\n",
       "      <th>convergence_q</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.2</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      p     q  alpha convergence_loss convergence_p convergence_q\n",
       "0  0.17  0.08    0.2               20            11            17\n",
       "0  0.40  0.20    0.4                4             5             4"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 26.83it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 31.89it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 29.51it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 32.38it/s]\n",
      "C:\\Users\\feder\\AppData\\Local\\Temp\\ipykernel_44752\\1130406705.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_sequential = pd.concat([df_sequential, tmp], axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence for p=0.17, q=0.08, alpha=0.2: loss=2, p=5, q=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 31.33it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 31.67it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 31.69it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 31.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence for p=0.4, q=0.2, alpha=0.4: loss=2, p=3, q=3\n"
     ]
    }
   ],
   "source": [
    "# repeat with sequential sampler\n",
    "df_sequential = pd.DataFrame(columns=['p', 'q', 'alpha', 'convergence_loss', 'convergence_p', 'convergence_q'])\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "# run the samplers (n_chains)\n",
    "\n",
    "sequential_p_list = np.zeros((len(param_list), n_iter, n_chains))\n",
    "sequential_q_list = np.zeros((len(param_list), n_iter, n_chains))\n",
    "sequential_loss_list = np.zeros((len(param_list), n_iter, n_chains))\n",
    "\n",
    "for (p,q,alpha) in param_list:\n",
    "    sbm = Homogeneous_SBM(n, k, p, q, Neal=False)\n",
    "    A = sbm.get_A()\n",
    "    z = sbm.get_z()\n",
    "    for a in range(n_chains):\n",
    "        z_init = warm_initializer(z, alpha, n, k)\n",
    "        sampler = sequential_Gibbs_sampler(A, z_init, alpha_p, beta_p, alpha_q, beta_q, pi_pri = pi)\n",
    "        sampler.run(n_iter)\n",
    "        # store the p_lists\n",
    "        sequential_p_list[param_list.index((p,q,alpha)), :, a] = sampler.get_p_list()\n",
    "        sequential_q_list[param_list.index((p,q,alpha)), :, a] = sampler.get_q_list()\n",
    "        sequential_loss_list[param_list.index((p,q,alpha)), :, a] = [loss(z, sampler.get_z_list()[i])[0] for i in range(n_iter)]\n",
    "\n",
    "    # compute R_hat for subsequence (1,r) for all r\n",
    "    sequential_R_hat_p = np.zeros(n_iter//2)\n",
    "    sequential_R_hat_q = np.zeros(n_iter//2)\n",
    "    sequential_R_hat_loss = np.zeros(n_iter//2)\n",
    "    idx = np.zeros(n_iter//2)\n",
    "\n",
    "    for r in range(1+splits, n_iter, 2):\n",
    "        # take the first r rows of the p_list and q_list\n",
    "        p_list_split = sequential_p_list[param_list.index((p,q,alpha)),:r+1, :]\n",
    "        q_list_split = sequential_q_list[param_list.index((p,q,alpha)),:r+1, :]\n",
    "        loss_list_split = sequential_loss_list[param_list.index((p,q,alpha)),:r+1, :]\n",
    "        # convert p_list_split from array to list of arrays (n_chains arrays)\n",
    "        p_list_split = [p_list_split[:, i] for i in range(n_chains)]\n",
    "        q_list_split = [q_list_split[:, i] for i in range(n_chains)]\n",
    "        loss_list_split = [loss_list_split[:, i] for i in range(n_chains)]\n",
    "        p_list_split = split_chains(p_list_split, splits)\n",
    "        q_list_split = split_chains(q_list_split, splits)\n",
    "        loss_list_split = split_chains(loss_list_split, splits)\n",
    "        sequential_R_hat_p[r//2] = R_hat(p_list_split)\n",
    "        sequential_R_hat_q[r//2] = R_hat(q_list_split)\n",
    "        sequential_R_hat_loss[r//2] = R_hat(loss_list_split)\n",
    "        idx[r//2] = r\n",
    "\n",
    "    # compute the convergence\n",
    "    conv_loss = find_convergence(sequential_R_hat_loss, lb, ub)\n",
    "    conv_p = find_convergence(sequential_R_hat_p, lb, ub)\n",
    "    conv_q = find_convergence(sequential_R_hat_q, lb, ub)\n",
    "\n",
    "\n",
    "    # # plot the R_hat for p and q in the same subplot, and the loss in a separate subplot\n",
    "    # plt.figure(figsize = (15,15))\n",
    "    # plt.subplot(2, 1, 1)\n",
    "    # plt.plot(idx[3:], sequential_R_hat_p[3:], label='R_hat_p')\n",
    "    # plt.plot(idx[3:], sequential_R_hat_q[3:], label='R_hat_q')\n",
    "    # plt.title('R_hat for p and q')\n",
    "    # plt.subplot(2, 1, 2)\n",
    "    # plt.plot(sequential_R_hat_loss[3:], label='loss')\n",
    "    # plt.title('loss')\n",
    "    # plt.show()\n",
    "\n",
    "    print(f'Convergence for p={p}, q={q}, alpha={alpha}: loss={conv_loss}, p={conv_p}, q={conv_q}')\n",
    "\n",
    "    tmp = pd.DataFrame({'p': p, 'q': q, 'alpha': alpha, 'convergence_loss': conv_loss, 'convergence_p': conv_p, 'convergence_q': conv_q}, index = [0])\n",
    "\n",
    "    df_sequential = pd.concat([df_sequential, tmp], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 26.80it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 27.02it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 28.80it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 25.21it/s]\n",
      "C:\\Users\\feder\\AppData\\Local\\Temp\\ipykernel_44752\\2576818073.py:64: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_neal_seq = pd.concat([df_neal_seq, tmp], axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence for p=0.17, q=0.08, alpha=0.2, gamma=0: loss=7, p=5, q=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 28.12it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 29.72it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 31.74it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 31.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence for p=0.4, q=0.2, alpha=0.4, gamma=0: loss=2, p=3, q=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 30.55it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 31.17it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 31.83it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 32.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence for p=0.17, q=0.08, alpha=0.2, gamma=0.1: loss=7, p=5, q=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 31.99it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 30.57it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 24.82it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 27.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence for p=0.4, q=0.2, alpha=0.4, gamma=0.1: loss=2, p=3, q=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 24.63it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 23.51it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 31.06it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 30.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence for p=0.17, q=0.08, alpha=0.2, gamma=1: loss=6, p=5, q=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 30.98it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 31.42it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 31.63it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 30.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence for p=0.4, q=0.2, alpha=0.4, gamma=1: loss=2, p=3, q=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 29.40it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 31.25it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 30.61it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 31.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence for p=0.17, q=0.08, alpha=0.2, gamma=10: loss=6, p=5, q=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 31.58it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 31.26it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 30.31it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 31.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence for p=0.4, q=0.2, alpha=0.4, gamma=10: loss=2, p=3, q=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 31.72it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 31.66it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 31.32it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 30.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence for p=0.17, q=0.08, alpha=0.2, gamma=10000: loss=5, p=5, q=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 31.60it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 30.68it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 31.92it/s]\n",
      "100%|██████████| 50/50 [00:01<00:00, 31.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence for p=0.4, q=0.2, alpha=0.4, gamma=10000: loss=2, p=3, q=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# repeat the same but for the neal sequential sampler\n",
    "df_neal_seq = pd.DataFrame(columns=['p', 'q', 'alpha', 'gamma', 'convergence_loss', 'convergence_p', 'convergence_q'])\n",
    "\n",
    "pic = False\n",
    "\n",
    "# run the samplers (n_chains)\n",
    "# batched\n",
    "neal_seq_p_list = np.zeros((len(gamma_list), len(param_list), n_iter, n_chains))\n",
    "neal_seq_q_list = np.zeros((len(gamma_list), len(param_list), n_iter, n_chains))  \n",
    "loss_list = np.zeros((len(gamma_list), len(param_list), n_iter, n_chains))\n",
    "\n",
    "\n",
    "\n",
    "for gamma in gamma_list:\n",
    "    np.random.seed(seed)\n",
    "    for (p,q,alpha) in param_list:\n",
    "        sbm = Homogeneous_SBM(n, k, p, q, Neal=False)\n",
    "        A = sbm.get_A()\n",
    "        z = sbm.get_z()\n",
    "        for a in range(n_chains):\n",
    "            z_init = warm_initializer(z, alpha, n, k)\n",
    "            sampler = Neal_sequential_Gibbs_sampler(A, z_init, alpha_p, beta_p, alpha_q, beta_q, gamma=gamma)\n",
    "            sampler.run(n_iter)\n",
    "            # store the p_lists\n",
    "            neal_seq_p_list[gamma_list.index(gamma), param_list.index((p,q,alpha)), :, a] = sampler.get_p_list()\n",
    "            neal_seq_q_list[gamma_list.index(gamma), param_list.index((p,q,alpha)), :, a] = sampler.get_q_list()\n",
    "            z_list = sampler.get_z_list()\n",
    "            loss_list[gamma_list.index(gamma), param_list.index((p,q,alpha)), :, a] = [loss(z, z_list[i])[0] for i in range(n_iter)]\n",
    "\n",
    "        # compute R_hat for subsequence (1,r) for all r\n",
    "        neal_seq_R_hat_p = np.zeros(n_iter//2)\n",
    "        neal_seq_R_hat_q = np.zeros(n_iter//2)\n",
    "        neal_seq_R_hat_loss = np.zeros(n_iter//2)\n",
    "        idx = np.zeros(n_iter//2)\n",
    "\n",
    "        for r in range(1+splits, n_iter, 2):\n",
    "            # take the first r rows of the p_list and q_list\n",
    "            p_list_split = neal_seq_p_list[gamma_list.index(gamma), param_list.index((p,q,alpha)), :r+1, :]\n",
    "            q_list_split = neal_seq_q_list[gamma_list.index(gamma), param_list.index((p,q,alpha)), :r+1, :]\n",
    "            loss_list_split = loss_list[gamma_list.index(gamma), param_list.index((p,q,alpha)), :r+1, :]\n",
    "            # convert p_list_split from array to list of arrays (n_chains arrays)\n",
    "            p_list_split = [p_list_split[:, i] for i in range(n_chains)]\n",
    "            q_list_split = [q_list_split[:, i] for i in range(n_chains)]\n",
    "            loss_list_split = [loss_list_split[:, i] for i in range(n_chains)]\n",
    "            p_list_split = split_chains(p_list_split, splits)\n",
    "            q_list_split = split_chains(q_list_split, splits)\n",
    "            loss_list_split = split_chains(loss_list_split, splits)\n",
    "            neal_seq_R_hat_p[r//2] = R_hat(p_list_split)\n",
    "            neal_seq_R_hat_q[r//2] = R_hat(q_list_split)\n",
    "            neal_seq_R_hat_loss[r//2] = R_hat(loss_list_split)\n",
    "            idx[r//2] = r\n",
    "\n",
    "        # average the loss over the chains\n",
    "        loss_avg = np.mean(loss_list[gamma_list.index(gamma), param_list.index((p,q,alpha)), :, :], axis=1)\n",
    "\n",
    "        # compute the convergence\n",
    "        conv_loss = find_convergence(neal_seq_R_hat_loss, lb, ub)\n",
    "        conv_p = find_convergence(neal_seq_R_hat_p, lb, ub)\n",
    "        conv_q = find_convergence(neal_seq_R_hat_q, lb, ub)\n",
    "\n",
    "        print(f'Convergence for p={p}, q={q}, alpha={alpha}, gamma={gamma}: loss={conv_loss}, p={conv_p}, q={conv_q}')\n",
    "\n",
    "        tmp = pd.DataFrame({'p': p, 'q': q, 'alpha': alpha, 'gamma': gamma, 'convergence_loss': conv_loss, 'convergence_p': conv_p, 'convergence_q': conv_q}, index = [0])\n",
    "        df_neal_seq = pd.concat([df_neal_seq, tmp], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 101.94it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 110.85it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 116.55it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 102.60it/s]\n",
      "C:\\Users\\feder\\AppData\\Local\\Temp\\ipykernel_44752\\2954739488.py:63: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_neal_batched = pd.concat([df_neal_batched, tmp], axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence for p=0.17, q=0.08, alpha=0.3, gamma=0: loss=13, p=10, q=9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 101.04it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 99.33it/s] \n",
      "100%|██████████| 50/50 [00:00<00:00, 111.77it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 114.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence for p=0.4, q=0.2, alpha=0.4, gamma=0: loss=7, p=6, q=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 102.21it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 110.19it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 111.34it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 92.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence for p=0.17, q=0.08, alpha=0.3, gamma=0.1: loss=13, p=10, q=9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 114.83it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 114.02it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 111.23it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 93.12it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence for p=0.4, q=0.2, alpha=0.4, gamma=0.1: loss=7, p=6, q=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 100.64it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 100.57it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 89.00it/s] \n",
      "100%|██████████| 50/50 [00:00<00:00, 68.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence for p=0.17, q=0.08, alpha=0.3, gamma=1: loss=13, p=10, q=9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 86.59it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 87.85it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 90.63it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 111.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence for p=0.4, q=0.2, alpha=0.4, gamma=1: loss=7, p=6, q=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 105.98it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 99.63it/s] \n",
      "100%|██████████| 50/50 [00:00<00:00, 109.82it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 109.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence for p=0.17, q=0.08, alpha=0.3, gamma=10: loss=13, p=11, q=9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 110.63it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 109.35it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 111.03it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 107.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence for p=0.4, q=0.2, alpha=0.4, gamma=10: loss=7, p=6, q=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 112.49it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 101.41it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 118.52it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 110.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence for p=0.17, q=0.08, alpha=0.3, gamma=10000: loss=13, p=10, q=9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 113.12it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 110.03it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 115.52it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 113.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence for p=0.4, q=0.2, alpha=0.4, gamma=10000: loss=7, p=6, q=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# repeat with the neal batched sampler\n",
    "\n",
    "df_neal_batched = pd.DataFrame(columns=['p', 'q', 'alpha', 'gamma', 'convergence_loss', 'convergence_p', 'convergence_q'])\n",
    "\n",
    "pic = False\n",
    "\n",
    "# run the samplers (n_chains)\n",
    "# batched\n",
    "neal_batched_p_list = np.zeros((len(gamma_list), len(param_list), n_iter, n_chains))\n",
    "neal_batched_q_list = np.zeros((len(gamma_list), len(param_list), n_iter, n_chains))\n",
    "loss_list = np.zeros((len(gamma_list), len(param_list), n_iter, n_chains))\n",
    "\n",
    "for gamma in gamma_list:\n",
    "    np.random.seed(seed)\n",
    "    for (p,q,alpha) in param_list:\n",
    "        sbm = Homogeneous_SBM(n, k, p, q, Neal=False)\n",
    "        A = sbm.get_A()\n",
    "        z = sbm.get_z()\n",
    "        for a in range(n_chains):\n",
    "            z_init = warm_initializer(z, alpha, n, k)\n",
    "            sampler = Neal_batched_Gibbs_sampler(A, z_init, alpha_p, beta_p, alpha_q, beta_q, gamma=gamma)\n",
    "            sampler.run(n_iter)\n",
    "            # store the p_lists\n",
    "            neal_batched_p_list[gamma_list.index(gamma), param_list.index((p,q,alpha)), :, a] = sampler.get_p_list()\n",
    "            neal_batched_q_list[gamma_list.index(gamma), param_list.index((p,q,alpha)), :, a] = sampler.get_q_list()\n",
    "            z_list = sampler.get_z_list()\n",
    "            loss_list[gamma_list.index(gamma), param_list.index((p,q,alpha)), :, a] = [loss(z, z_list[i])[0] for i in range(n_iter)]\n",
    "\n",
    "        # compute R_hat for subsequence (1,r) for all r\n",
    "        neal_batched_R_hat_p = np.zeros(n_iter//2)\n",
    "        neal_batched_R_hat_q = np.zeros(n_iter//2)\n",
    "        neal_batched_R_hat_loss = np.zeros(n_iter//2)\n",
    "        idx = np.zeros(n_iter//2)\n",
    "\n",
    "        for r in range(1+splits, n_iter, 2):\n",
    "            # take the first r rows of the p_list and q_list\n",
    "            p_list_split = neal_batched_p_list[gamma_list.index(gamma), param_list.index((p,q,alpha)), :r+1, :]\n",
    "            q_list_split = neal_batched_q_list[gamma_list.index(gamma), param_list.index((p,q,alpha)), :r+1, :]\n",
    "            loss_list_split = loss_list[gamma_list.index(gamma), param_list.index((p,q,alpha)), :r+1, :]\n",
    "            # convert p_list_split from array to list of arrays (n_chains arrays)\n",
    "            p_list_split = [p_list_split[:, i] for i in range(n_chains)]\n",
    "            q_list_split = [q_list_split[:, i] for i in range(n_chains)]\n",
    "            loss_list_split = [loss_list_split[:, i] for i in range(n_chains)]\n",
    "            p_list_split = split_chains(p_list_split, splits)\n",
    "            q_list_split = split_chains(q_list_split, splits)\n",
    "            loss_list_split = split_chains(loss_list_split, splits)\n",
    "            neal_batched_R_hat_p[r//2] = R_hat(p_list_split)\n",
    "            neal_batched_R_hat_q[r//2] = R_hat(q_list_split)\n",
    "            neal_batched_R_hat_loss[r//2] = R_hat(loss_list_split)\n",
    "            idx[r//2] = r\n",
    "\n",
    "        # average the loss over the chains\n",
    "        loss_avg = np.mean(loss_list[gamma_list.index(gamma), param_list.index((p,q,alpha)), :], axis=1)\n",
    "\n",
    "        # compute the convergence\n",
    "        conv_loss = find_convergence(neal_batched_R_hat_loss, lb, ub)\n",
    "        conv_p = find_convergence(neal_batched_R_hat_p, lb, ub)\n",
    "        conv_q = find_convergence(neal_batched_R_hat_q, lb, ub)\n",
    "\n",
    "        print(f'Convergence for p={p}, q={q}, alpha={alpha}, gamma={gamma}: loss={conv_loss}, p={conv_p}, q={conv_q}')\n",
    "\n",
    "        tmp = pd.DataFrame({'p': p, 'q': q, 'alpha': alpha, 'gamma': gamma, 'convergence_loss': conv_loss, 'convergence_p': conv_p, 'convergence_q': conv_q}, index = [0])\n",
    "        df_neal_batched = pd.concat([df_neal_batched, tmp], axis=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\feder\\AppData\\Local\\Temp\\ipykernel_44752\\3162828709.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_weak[\"avg\"] = avg\n",
      "C:\\Users\\feder\\AppData\\Local\\Temp\\ipykernel_44752\\3162828709.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_strong[\"avg\"] = avg\n"
     ]
    }
   ],
   "source": [
    "# combine dataframes\n",
    "df_batched['sampler'] = 'batched'\n",
    "df_sequential['sampler'] = 'sequential'\n",
    "df_neal_seq['sampler'] = 'neal_seq'\n",
    "df_neal_batched['sampler'] = 'neal_batched'\n",
    "\n",
    "# add a null gamma row to batched and sequential\n",
    "df_batched['gamma'] = None\n",
    "df_sequential['gamma'] = None\n",
    "\n",
    "df = pd.concat([df_batched, df_sequential, df_neal_seq, df_neal_batched], axis=0)\n",
    "# filter the alpha = 0.2\n",
    "df_weak = df[df['alpha'] == 0.3]\n",
    "df_strong = df[df['alpha'] == 0.4]\n",
    "\n",
    "# sort them by the average of the convergence\n",
    "avg = df_weak[['convergence_loss', 'convergence_p', 'convergence_q']].mean(axis = 1)\n",
    "df_weak[\"avg\"] = avg\n",
    "avg = df_strong[['convergence_loss', 'convergence_p', 'convergence_q']].mean(axis = 1)\n",
    "df_strong[\"avg\"] = avg\n",
    "\n",
    "df_weak = df_weak.sort_values(by='avg')\n",
    "df_strong = df_strong.sort_values(by='avg')\n",
    "\n",
    "# take also the max\n",
    "max_val = df_weak[['convergence_loss', 'convergence_p', 'convergence_q']].max(axis = 1)\n",
    "df_weak[\"max\"] = max_val\n",
    "max_val = df_strong[['convergence_loss', 'convergence_p', 'convergence_q']].max(axis = 1)\n",
    "df_strong[\"max\"] = max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>q</th>\n",
       "      <th>alpha</th>\n",
       "      <th>convergence_loss</th>\n",
       "      <th>convergence_p</th>\n",
       "      <th>convergence_q</th>\n",
       "      <th>sampler</th>\n",
       "      <th>gamma</th>\n",
       "      <th>avg</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>sequential</td>\n",
       "      <td>None</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>neal_seq</td>\n",
       "      <td>10</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>neal_seq</td>\n",
       "      <td>10000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>neal_seq</td>\n",
       "      <td>0</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>neal_seq</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>neal_seq</td>\n",
       "      <td>1</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.3</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>batched</td>\n",
       "      <td>None</td>\n",
       "      <td>10.666667</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.3</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>neal_batched</td>\n",
       "      <td>0</td>\n",
       "      <td>10.666667</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.3</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>neal_batched</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10.666667</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.3</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>neal_batched</td>\n",
       "      <td>1</td>\n",
       "      <td>10.666667</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.3</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>neal_batched</td>\n",
       "      <td>10000</td>\n",
       "      <td>10.666667</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.3</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>neal_batched</td>\n",
       "      <td>10</td>\n",
       "      <td>11.0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      p     q  alpha convergence_loss convergence_p convergence_q  \\\n",
       "0  0.17  0.08    0.3                2             5             5   \n",
       "0  0.17  0.08    0.3                2             5             5   \n",
       "0  0.17  0.08    0.3                2             5             5   \n",
       "0  0.17  0.08    0.3                2             5             6   \n",
       "0  0.17  0.08    0.3                2             5             6   \n",
       "0  0.17  0.08    0.3                2             5             6   \n",
       "0  0.17  0.08    0.3               13            10             9   \n",
       "0  0.17  0.08    0.3               13            10             9   \n",
       "0  0.17  0.08    0.3               13            10             9   \n",
       "0  0.17  0.08    0.3               13            10             9   \n",
       "0  0.17  0.08    0.3               13            10             9   \n",
       "0  0.17  0.08    0.3               13            11             9   \n",
       "\n",
       "        sampler  gamma        avg max  \n",
       "0    sequential   None        4.0   5  \n",
       "0      neal_seq     10        4.0   5  \n",
       "0      neal_seq  10000        4.0   5  \n",
       "0      neal_seq      0   4.333333   6  \n",
       "0      neal_seq    0.1   4.333333   6  \n",
       "0      neal_seq      1   4.333333   6  \n",
       "0       batched   None  10.666667  13  \n",
       "0  neal_batched      0  10.666667  13  \n",
       "0  neal_batched    0.1  10.666667  13  \n",
       "0  neal_batched      1  10.666667  13  \n",
       "0  neal_batched  10000  10.666667  13  \n",
       "0  neal_batched     10       11.0  13  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_weak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>q</th>\n",
       "      <th>alpha</th>\n",
       "      <th>convergence_loss</th>\n",
       "      <th>convergence_p</th>\n",
       "      <th>convergence_q</th>\n",
       "      <th>sampler</th>\n",
       "      <th>gamma</th>\n",
       "      <th>avg</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>sequential</td>\n",
       "      <td>None</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>neal_seq</td>\n",
       "      <td>10000</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>neal_seq</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>neal_seq</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>neal_seq</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>neal_seq</td>\n",
       "      <td>10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>batched</td>\n",
       "      <td>None</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>neal_batched</td>\n",
       "      <td>0</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>neal_batched</td>\n",
       "      <td>0.1</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>neal_batched</td>\n",
       "      <td>1</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>neal_batched</td>\n",
       "      <td>10</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>neal_batched</td>\n",
       "      <td>10000</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     p    q  alpha convergence_loss convergence_p convergence_q       sampler  \\\n",
       "0  0.4  0.2    0.4                2             3             3    sequential   \n",
       "0  0.4  0.2    0.4                2             3             3      neal_seq   \n",
       "0  0.4  0.2    0.4                2             4             3      neal_seq   \n",
       "0  0.4  0.2    0.4                2             4             3      neal_seq   \n",
       "0  0.4  0.2    0.4                2             4             3      neal_seq   \n",
       "0  0.4  0.2    0.4                2             4             3      neal_seq   \n",
       "0  0.4  0.2    0.4                7             6             6       batched   \n",
       "0  0.4  0.2    0.4                7             6             6  neal_batched   \n",
       "0  0.4  0.2    0.4                7             6             6  neal_batched   \n",
       "0  0.4  0.2    0.4                7             6             6  neal_batched   \n",
       "0  0.4  0.2    0.4                7             6             6  neal_batched   \n",
       "0  0.4  0.2    0.4                7             6             6  neal_batched   \n",
       "\n",
       "   gamma       avg max  \n",
       "0   None  2.666667   3  \n",
       "0  10000  2.666667   3  \n",
       "0      0       3.0   4  \n",
       "0    0.1       3.0   4  \n",
       "0      1       3.0   4  \n",
       "0     10       3.0   4  \n",
       "0   None  6.333333   7  \n",
       "0      0  6.333333   7  \n",
       "0    0.1  6.333333   7  \n",
       "0      1  6.333333   7  \n",
       "0     10  6.333333   7  \n",
       "0  10000  6.333333   7  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_strong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra = False\n",
    "if extra:\n",
    "    pq_list = [(0.17, 0.08), (0.4, 0.2)]\n",
    "    alpha_list = [0.2, 0.3, 0.4]\n",
    "    n_iter = 150\n",
    "    burn_in = 0\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    convergence_lb = 0.9\n",
    "    convergence_ub = 1.1\n",
    "\n",
    "    # convergence iteration (store for each combination the iteration where R_hat is in the interval\n",
    "    convergence_iter = np.zeros((len(pq_list), len(alpha_list), len(gamma_list)+1))\n",
    "    # seed\n",
    "    np.random.seed(0)\n",
    "\n",
    "    pic = False\n",
    "\n",
    "    # for over the lists above\n",
    "    for (p,q) in pq_list:\n",
    "        for alpha in alpha_list:\n",
    "            sbm = Homogeneous_SBM(n, k, p, q, Neal=False)\n",
    "            A = sbm.get_A()\n",
    "            z = sbm.get_z()\n",
    "            # run the samplers (n_chains)\n",
    "            # batched\n",
    "            batched_p_list = np.zeros((n_iter, n_chains))\n",
    "            batched_q_list = np.zeros((n_iter, n_chains))\n",
    "            loss_list = np.zeros((n_iter, n_chains))\n",
    "            for a in range(n_chains):\n",
    "                z_init = warm_initializer(z, alpha, n, k)\n",
    "                sampler = batched_Gibbs_sampler(A, z_init, alpha_p, beta_p, alpha_q, beta_q, pi_pri = pi)\n",
    "                sampler.run(n_iter)\n",
    "                # store the p_lists\n",
    "                batched_p_list[:, a] = sampler.get_p_list()\n",
    "                batched_q_list[:, a] = sampler.get_q_list()\n",
    "                z_list = sampler.get_z_list()\n",
    "                loss_list[:, a] = [loss(z, z_list[i])[0] for i in range(n_iter)]\n",
    "\n",
    "            # compute R_hat for subsequence (1,r) for all r\n",
    "            batched_R_hat_p = np.zeros(n_iter//2)\n",
    "            batched_R_hat_q = np.zeros(n_iter//2)\n",
    "            idx = np.zeros(n_iter//2)\n",
    "\n",
    "            for r in range(1+splits, n_iter, 2):\n",
    "                # take the first r rows of the p_list and q_list\n",
    "                p_list_split = batched_p_list[:r+1, :]\n",
    "                q_list_split = batched_q_list[:r+1, :]\n",
    "                # convert p_list_split from array to list of arrays (n_chains arrays)\n",
    "                p_list_split = [p_list_split[:, i] for i in range(n_chains)]\n",
    "                q_list_split = [q_list_split[:, i] for i in range(n_chains)]\n",
    "                p_list_split = split_chains(p_list_split, splits)\n",
    "                q_list_split = split_chains(q_list_split, splits)\n",
    "                batched_R_hat_p[r//2] = R_hat(p_list_split)\n",
    "                batched_R_hat_q[r//2] = R_hat(q_list_split)\n",
    "                idx[r//2] = r\n",
    "\n",
    "            # average the loss over the chains\n",
    "            loss_avg = np.mean(loss_list, axis=1)\n",
    "\n",
    "            conv_p = find_convergence(batched_R_hat_p, convergence_lb, convergence_ub)\n",
    "            conv_q = find_convergence(batched_R_hat_q, convergence_lb, convergence_ub)\n",
    "            convergence_iter[pq_list.index((p,q)), alpha_list.index(alpha), 0] = max(conv_p, conv_q)*2\n",
    "            print(\"Convergence iteration: {}\".format(convergence_iter[pq_list.index((p,q)), alpha_list.index(alpha), 0]))\n",
    "\n",
    "            print('batched, p: {}, q: {}, alpha: {}'.format(p, q, alpha))\n",
    "            # plot the R_hat for p and q in the same subplot, and the loss in a separate subplot\n",
    "            plt.figure(figsize = (15,15))\n",
    "            plt.subplot(2, 1, 1)\n",
    "            plt.plot(idx, batched_R_hat_p, label='R_hat_p')\n",
    "            plt.plot(idx, batched_R_hat_q, label='R_hat_q')\n",
    "            # add the convergence bounds\n",
    "            plt.axhline(y=convergence_lb, color='r', linestyle='--')\n",
    "            plt.axhline(y=convergence_ub, color='r', linestyle='--')\n",
    "            # vertical line at the convergence iteration\n",
    "            plt.axvline(x=convergence_iter[pq_list.index((p,q)), alpha_list.index(alpha), 0], color='g', linestyle='--')\n",
    "            plt.title('R_hat for p and q')\n",
    "            plt.subplot(2, 1, 2)\n",
    "            plt.plot(loss_avg, label='loss')\n",
    "            plt.title('loss')\n",
    "            if pic:\n",
    "                plt.savefig('speed/p{}_q{}_alpha{}.png'.format(p, q, alpha))\n",
    "            plt.show()\n",
    "\n",
    "            for gamma in gamma_list:\n",
    "                # run the samplers (n_chains)\n",
    "                # batched\n",
    "                batched_p_list = np.zeros((n_iter, n_chains))\n",
    "                batched_q_list = np.zeros((n_iter, n_chains))\n",
    "                loss_list = np.zeros((n_iter, n_chains))\n",
    "                for a in range(n_chains):\n",
    "                    z_init = warm_initializer(z, alpha, n, k)\n",
    "                    sampler = Neal_batched_Gibbs_sampler(A, z_init, alpha_p, beta_p, alpha_q, beta_q, gamma = gamma)\n",
    "                    sampler.run(n_iter)\n",
    "                    # store the p_lists\n",
    "                    batched_p_list[:, a] = sampler.get_p_list()\n",
    "                    batched_q_list[:, a] = sampler.get_q_list()\n",
    "                    z_list = sampler.get_z_list()\n",
    "                    loss_list[:, a] = [loss(z, z_list[i])[0] for i in range(n_iter)]\n",
    "\n",
    "                # compute R_hat for subsequence (1,r) for all r\n",
    "                batched_R_hat_p = np.zeros(n_iter//2)\n",
    "                batched_R_hat_q = np.zeros(n_iter//2)\n",
    "                idx = np.zeros(n_iter//2)\n",
    "\n",
    "                for r in range(1+splits, n_iter, 2):\n",
    "                    # take the first r rows of the p_list and q_list\n",
    "                    p_list_split = batched_p_list[:r+1, :]\n",
    "                    q_list_split = batched_q_list[:r+1, :]\n",
    "                    # convert p_list_split from array to list of arrays (n_chains arrays)\n",
    "                    p_list_split = [p_list_split[:, i] for i in range(n_chains)]\n",
    "                    q_list_split = [q_list_split[:, i] for i in range(n_chains)]\n",
    "                    p_list_split = split_chains(p_list_split, splits)\n",
    "                    q_list_split = split_chains(q_list_split, splits)\n",
    "                    batched_R_hat_p[r//2] = R_hat(p_list_split)\n",
    "                    batched_R_hat_q[r//2] = R_hat(q_list_split)\n",
    "                    idx[r//2] = r\n",
    "\n",
    "                # average the loss over the chains\n",
    "                loss_avg = np.mean(loss_list, axis=1)\n",
    "                \n",
    "                conv_p = find_convergence(batched_R_hat_p, convergence_lb, convergence_ub)\n",
    "                conv_q = find_convergence(batched_R_hat_q, convergence_lb, convergence_ub)\n",
    "                convergence_iter[pq_list.index((p,q)), alpha_list.index(alpha), gamma_list.index(gamma)+1] = max(conv_p, conv_q)*2\n",
    "                print(\"Convergence iteration: {}\".format(convergence_iter[pq_list.index((p,q)), alpha_list.index(alpha), gamma_list.index(gamma)+1]))\n",
    "\n",
    "                print(\"Neal, batched, p: {}, q: {}, alpha: {}, gamma: {}\".format(p, q, alpha, gamma))\n",
    "                # plot the R_hat for p and q in the same subplot, and the loss in a separate subplot\n",
    "                plt.figure(figsize = (15,15))\n",
    "                plt.subplot(2, 1, 1)\n",
    "                plt.plot(idx, batched_R_hat_p, label='R_hat_p')\n",
    "                plt.plot(idx, batched_R_hat_q, label='R_hat_q')\n",
    "                # add the convergence bounds\n",
    "                plt.axhline(y=convergence_lb, color='r', linestyle='--')\n",
    "                plt.axhline(y=convergence_ub, color='r', linestyle='--')\n",
    "                # vertical line at the convergence iteration\n",
    "                plt.axvline(x=convergence_iter[pq_list.index((p,q)), alpha_list.index(alpha), gamma_list.index(gamma)+1], color='g', linestyle='--')\n",
    "                plt.title('R_hat for p and q')\n",
    "                plt.subplot(2, 1, 2)\n",
    "                plt.plot(loss_avg, label='loss')\n",
    "                plt.title('loss')\n",
    "                if pic:\n",
    "                    plt.savefig('speed/neal_p{}_q{}_alpha{}_gamma{}.png'.format(p, q, alpha, gamma))\n",
    "                plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if extra:\n",
    "    # for each alpha, plot the convergence iteration as a function of gamma (use vertical lines for each point, not a line)\n",
    "    pic=False\n",
    "    for alpha in alpha_list:\n",
    "        plt.figure(figsize = (15,15))\n",
    "        for (p,q) in pq_list:\n",
    "            idx = np.arange(len(gamma_list)+1)\n",
    "            plt.plot(idx, convergence_iter[pq_list.index((p,q)), alpha_list.index(alpha), :], label='p: {}, q: {}'.format(p,q))\n",
    "        plt.title('Convergence iteration for alpha = {}'.format(alpha))\n",
    "        plt.xlabel('gamma')\n",
    "        plt.ylabel('convergence iteration')\n",
    "        plt.xticks(idx)\n",
    "        plt.legend()\n",
    "        if pic:\n",
    "            plt.savefig('r_hat/convergence_iteration_alpha{}.png'.format(alpha))\n",
    "        plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
